!pip install openpyxl xgboost

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold
from xgboost import XGBClassifier
from google.colab import files

uploaded = files.upload()

for file_name in uploaded.keys():
    if file_name.endswith('.csv'):
        df = pd.read_csv(file_name)
    elif file_name.endswith(('.xls', '.xlsx')):
        df = pd.read_excel(file_name)
    else:
        raise ValueError("Unsupported file type")

target_column = 'Exited'
columns_to_drop = ['RowNumber', 'CustomerId', 'Surname']
df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])

X = df.drop(target_column, axis=1)
y = df[target_column]

numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_features = X.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('variance_filter', VarianceThreshold(threshold=0.0)),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(transformers=[
    ('num', numeric_transformer, numeric_features),
    ('cat', categorical_transformer, categorical_features)
])

models = {
    'Random Forest': RandomForestClassifier(random_state=42, class_weight='balanced'),
    'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),
    'AdaBoost': AdaBoostClassifier(random_state=42)
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']
best_model = None
best_score = 0
best_name = ""
results_dict = {}

for name, clf in models.items():
    print(f"\n=== {name} ===")
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', clf)
    ])
    
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    y_proba = pipeline.predict_proba(X_test)[:, 1]

    print(classification_report(y_test, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    print("ROC AUC Score (Test Set):", roc_auc_score(y_test, y_proba))

    cv_results = cross_validate(pipeline, X, y, cv=cv, scoring=scoring, return_train_score=False)
    mean_roc_auc = np.mean(cv_results['test_roc_auc'])
    results_dict[name] = mean_roc_auc

    for metric in scoring:
        mean_score = np.mean(cv_results[f'test_{metric}'])
        print(f"{metric.capitalize()} (mean CV): {mean_score:.4f}")
    
    if mean_roc_auc > best_score:
        best_score = mean_roc_auc
        best_model = pipeline
        best_name = name
    
    if hasattr(pipeline.named_steps['classifier'], 'feature_importances_'):
        fitted_preprocessor = pipeline.named_steps['preprocessor']
        ohe = fitted_preprocessor.named_transformers_['cat'].named_steps['onehot']
        cat_feature_names = ohe.get_feature_names_out(categorical_features)
        feature_names = numeric_features + list(cat_feature_names)
        importances = pipeline.named_steps['classifier'].feature_importances_

        importance_df = pd.DataFrame({
            'Feature': feature_names,
            'Importance': importances
        }).sort_values(by='Importance', ascending=False)

        plt.figure(figsize=(10, 6))
        plt.barh(importance_df['Feature'][:15][::-1], importance_df['Importance'][:15][::-1], color='skyblue')
        plt.xlabel('Feature Importance')
        plt.title(f'Top 15 Important Features - {name}')
        plt.tight_layout()
        plt.show()

print("\n")
print(f" Best Model: {best_name} (CV ROC AUC: {best_score:.4f})")

